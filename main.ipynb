{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank account fraud detection\n",
    "Daniel Mizrahi (10675418), Antonio La Chira Marquina (11847018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_analysis import *\n",
    "from read_data import read_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the data\n",
    "We first start off by reading in the data. Since some of the data is categorical and therefore non-numerical we must use one hot encoding to process the data and make sure this won't be a problem when implenting our classifiers. Altough perhaps marginal when converting categorical data we drop the first column so there is less data later on to perform calculations on. The original data without one hot encoding can be used to make conclusions about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from read_data import read_dataset\n",
    "\n",
    "data_original = read_dataset('datasets/Base.csv', process=False)\n",
    "data = read_dataset('datasets/Base.csv', process=True, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Preview of the one hot encoded data in table form (first 10 rows and 4 columns):')\n",
    "print(data.head(10).iloc[:, :4])\n",
    "\n",
    "print(f'\\nOrignal data shape (rows, columns): {data_original.shape}')\n",
    "print(f'One hot encoded data shape (rows, columns): {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# k = 'auto'\n",
    "k = 50\n",
    "\n",
    "categorical_data = ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n",
    "boolean_data = ['fraud_bool', 'email_is_free', 'phone_home_valid', 'phone_mobile_valid', 'has_other_cards', 'foreign_request', 'keep_alive_session', 'device_distinct_emails_8w', 'device_fraud_count']\n",
    "\n",
    "fig, axs = plt.subplots(16, 2, figsize=(16,100))\n",
    "column_names = list(data_original.columns.values)\n",
    "for column, ax in enumerate(axs.flat):\n",
    "    ax.set_xlabel(column_names[column])\n",
    "    ax.set_ylabel('Count')\n",
    "    if column_names[column] in boolean_data:\n",
    "        labels, counts = np.unique(data_original.iloc[:, column], return_counts=True)\n",
    "        ax.bar(labels, counts, width=1)\n",
    "        ax.set_xticks(labels)\n",
    "        ax.set_xticklabels(['False', 'True'])\n",
    "    elif column_names[column] in categorical_data:\n",
    "        labels, counts = np.unique(data_original.iloc[:, column], return_counts=True)\n",
    "        ax.bar(labels, counts)\n",
    "    else:\n",
    "        ax.hist(data_original.iloc[:, column], k)\n",
    "    if column_names[column] not in categorical_data + boolean_data:\n",
    "        mean = np.mean(data_original.iloc[:, column])\n",
    "        standard_deviation = np.std(data_original.iloc[:, column])\n",
    "        ax.axvline(mean, color='g')\n",
    "        if mean - standard_deviation > np.amin(data_original.iloc[:, column]):\n",
    "            ax.axvline(mean - standard_deviation, color='r')\n",
    "        ax.axvline(mean + standard_deviation, color='r')\n",
    "        ax.legend([f'mean (approx {round(mean, 2)})', f'standard deviation (approx {round(standard_deviation, 2)})'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# k = 'auto'\n",
    "k = 50\n",
    "\n",
    "categorical_data = ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']\n",
    "boolean_data = ['fraud_bool', 'email_is_free', 'phone_home_valid', 'phone_mobile_valid', 'has_other_cards', 'foreign_request', 'keep_alive_session', 'device_distinct_emails_8w', 'device_fraud_count']\n",
    "\n",
    "data_fraud = data_original.loc[data_original['fraud_bool'] == 1]\n",
    "data_no_fraud = data_original.loc[data_original['fraud_bool'] == 0]\n",
    "\n",
    "fig, axs = plt.subplots(16, 2, figsize=(16,100))\n",
    "column_names = list(data_original.columns.values)\n",
    "for column, ax in enumerate(axs.flat):\n",
    "    ax.set_xlabel(column_names[column])\n",
    "    ax.set_ylabel('Count')\n",
    "    if column_names[column] in boolean_data:\n",
    "        lbls_f, cts_f = np.unique(data_fraud.iloc[:, column], return_counts=True)\n",
    "        lbls_nf, cts_nf = np.unique(data_no_fraud.iloc[:, column], return_counts=True)\n",
    "        ax.bar(lbls_f, cts_f / len(data_fraud.iloc[:, column]), width=1, label='Fraud', alpha=.6)\n",
    "        ax.bar(lbls_nf, cts_nf / len(data_no_fraud.iloc[:, column]), width=1, label='No fraud', alpha=.6)\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_xticklabels(['False', 'True'])\n",
    "    elif column_names[column] in categorical_data:\n",
    "        lbls_f, cts_f = np.unique(data_fraud.iloc[:, column], return_counts=True)\n",
    "        lbls_nf, cts_nf = np.unique(data_no_fraud.iloc[:, column], return_counts=True)\n",
    "        ax.bar(lbls_f, cts_f / len(data_fraud.iloc[:, column]), label='Fraud', alpha=.6)\n",
    "        ax.bar(lbls_nf, cts_nf / len(data_no_fraud.iloc[:, column]), label='No fraud', alpha=.6)\n",
    "    else:\n",
    "        ax.hist(data_fraud.iloc[:, column], k, label='Fraud', density=True, alpha=.6)\n",
    "        ax.hist(data_no_fraud.iloc[:, column], k, label='No fraud', density=True, alpha=.6)\n",
    "    ax.legend()\n",
    "    if column_names[column] not in categorical_data + boolean_data:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        mean_f = np.mean(data_fraud.iloc[:, column])\n",
    "        mean_nf = np.mean(data_no_fraud.iloc[:, column])\n",
    "        mn_f = ax.axvline(mean_f, color='g')\n",
    "        mn_nf = ax.axvline(mean_nf, color='k')\n",
    "        ax.legend(handles + [mn_f, mn_nf], labels + [f'mean fraud (approx {round(mean_f, 2)})', f'mean no fraud(approx {round(mean_nf, 2)})'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity of variables between fraudulent and nonfraudulant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KS: Note that the two-sample test checks whether the two data samples come from the same distribution.\n",
    "    This does not specify what that common distribution is (e.g. whether it's normal or not normal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ks_statistics = kolmogorov_smirnov_similarity(data_fraud, data_no_fraud)\n",
    "# ep_statistics = epps_singleton_similarity(data_fraud, data_no_fraud)\n",
    "ks_most_similar = np.argsort(-ks_statistics)\n",
    "# ep_most_similar = np.argsort(-ep_statistics)\n",
    "\n",
    "print('Kolmogorov-Smirnov similarity sorted descending (1.0 means complete dissimilarity and 0.0 complete similarity):')\n",
    "ks_table = list()\n",
    "for column in ks_most_similar:\n",
    "    # print(f'{data.columns[column]}: {ks_statistics[column]}')\n",
    "    ks_table.append([data.columns[column], ks_statistics[column]])\n",
    "print(pd.DataFrame(ks_table, columns=['Variable', 'KS statistic']))\n",
    "\n",
    "\n",
    "# print('\\nEpps-Singleton similarity sorted descending (1.0 means complete dissimilarity and 0.0 complete similarity):')\n",
    "# ep_table = list()\n",
    "# for column in ep_most_similar:\n",
    "#     ep_table.append([data.columns[column + 1], ep_statistics[column]])\n",
    "# print(pd.DataFrame(ep_table, columns=['Variable', 'EP statistic']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "pearson_correlations = pearson_correlation_coefficient(data)\n",
    "spearman_correlations = spearman_correlation_coefficient(data)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(24,40))\n",
    "\n",
    "hm_p = ax[0].imshow(np.array(pearson_correlations), cmap=plt.cm.rainbow)\n",
    "hm_s = ax[1].imshow(np.array(spearman_correlations), cmap=plt.cm.rainbow)\n",
    "\n",
    "ax[0].set_title('Heatmap of pearson correlations between all parameters', fontsize=20)\n",
    "ax[0].set_xticks(np.arange(len(data.columns)))\n",
    "ax[0].set_yticks(np.arange(len(data.columns)))\n",
    "\n",
    "ax[0].set_xticklabels(data.columns, rotation=90)\n",
    "ax[0].set_yticklabels(data.columns)\n",
    "\n",
    "ax[1].set_title('Heatmap of spearman correlations between all parameters', fontsize=20)\n",
    "ax[1].set_xticks(np.arange(len(data.columns)))\n",
    "ax[1].set_yticks(np.arange(len(data.columns)))\n",
    "\n",
    "ax[1].set_xticklabels(data.columns, rotation=90)\n",
    "ax[1].set_yticklabels(data.columns)\n",
    "\n",
    "# Add axes underneath both subplots\n",
    "div1 = make_axes_locatable(ax[0])\n",
    "div2 = make_axes_locatable(ax[1])\n",
    "\n",
    "cax1 = div1.new_vertical(size='5%', pad=2.5, pack_start=True)\n",
    "cax2 = div2.new_vertical(size='5%', pad=2.5, pack_start=True)\n",
    "\n",
    "fig.add_axes(cax1)\n",
    "fig.add_axes(cax2)\n",
    "\n",
    "fig.colorbar(hm_p, cax=cax1, orientation='horizontal')\n",
    "fig.colorbar(hm_s, cax=cax2, orientation='horizontal')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select strongest correlation parameters (pos)\n",
    "params = ['fraud_bool', 'device_os_windows', 'credit_risk_score', 'proposed_credit_limit', 'customer_age']\n",
    "\n",
    "log_reg_df = data[params]\n",
    "i_row = np.random.choice(log_reg_df.values.shape[0], 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from classifiers import *\n",
    "\n",
    "# fraud_bool column\n",
    "target_sample = log_reg_df.values[i_row, 0]\n",
    "train_sample = log_reg_df.values[i_row, 1:]\n",
    "\n",
    "clf_log = LogisticRegressionClassifier()\n",
    "clf_nb = NaiveBayesClassifier()\n",
    "\n",
    "clf_log.fit(train_sample, target_sample)\n",
    "clf_nb.fit(train_sample, target_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test ####\n",
    "# Fraud\n",
    "test = pd.DataFrame([\n",
    "       {\n",
    "           'device_os_windows' : 1,\n",
    "           'credit_risk_score' : 100,\n",
    "           'proposed_credit_limit' : 200,\n",
    "           'customer_age' : 50\n",
    "       }])\n",
    "\n",
    "print(test)\n",
    "\n",
    "clf.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
